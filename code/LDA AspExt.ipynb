{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/local/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "np.random.seed(2018)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('data/reviews_Grocery_and_Gourmet_Food_5/aspect_Grocery_and_Gourmet_Food_5_reviewsentences.csv', names = ['Chat Aspects'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89722"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CleanUp(sentence):\n",
    "    #no_punc = [ch for ch in sentence if ch not in string.punctuation]\n",
    "    #no_punc = [ch if ch not in string.punctuation else ' ' for ch in sentence]\n",
    "    no_punc = [ch if (ch not in string.punctuation and ch.isdigit()==False) else ' ' for ch in sentence]\n",
    "    no_punc = re.sub(' +',' ',''.join(no_punc).lower().strip())\n",
    "    no_stop = [word for word in no_punc.split() if (word not in stopwords.words('english') and word.isdigit()==False)]\n",
    "    ps = PorterStemmer()\n",
    "    stem_word = [ps.stem(word) for word in no_stop]\n",
    "    return(stem_word)\n",
    "    #return(' '.join(stem_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cf2470169af4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomments_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Chat Aspects'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCleanUp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcomments_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/local/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-25d36d62e607>\u001b[0m in \u001b[0;36mCleanUp\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#no_punc = [ch for ch in sentence if ch not in string.punctuation]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#no_punc = [ch if ch not in string.punctuation else ' ' for ch in sentence]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mno_punc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mno_punc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' +'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_punc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mno_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mno_punc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "comments_final = comments['Chat Aspects'].apply(CleanUp)\n",
    "comments_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               standard milk chocolate kit kat\n",
       "1                   The green tea flavor subtle\n",
       "2                       among Kit Kat fans well\n",
       "3                   The green tea flavor subtle\n",
       "4          green tea candies I tried. But still\n",
       "5                      best Kit Kats I 've ever\n",
       "6             I 've purchased different curries\n",
       "7       Because Mae Ploy Thai Green Curry Paste\n",
       "8              These dyes create awesome colors\n",
       "9             couple drops really vibrant color\n",
       "10                   natural / organic-I wish I\n",
       "11      nice basic selection gel food colorings\n",
       "12               The product seems high quality\n",
       "13                      best energy bar I tried\n",
       "14      better average tasting sports bar/snack\n",
       "15         I 've listed nutritional information\n",
       "16    PowerBar flavors well. Review M.  Reynard\n",
       "17        PowerBar part regular morning routine\n",
       "18          A couple times I 've asthma attacks\n",
       "19                    high protein bars I tried\n",
       "Name: Chat Aspects, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_final.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551219"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_docs = list(comments_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2ea528ae9df4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/local/anaconda3/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/local/anaconda3/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# update Dictionary with the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         logger.info(\n",
      "\u001b[0;32m/app/local/anaconda3/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \"\"\"\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(comments_final)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e5fbae7e12eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#dictionary.filter_extremes(no_below=100, no_above=0.5, keep_n=150000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_extremes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_below\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_above\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "#dictionary.filter_extremes(no_below=100, no_above=0.5, keep_n=150000)\n",
    "dictionary.filter_extremes(no_below=300, no_above=0.5, keep_n=150000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "#bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 31 (\"call\") appears 1 time.\n",
      "Word 77 (\"intern\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.8018761575907541), (1, 0.5974902743036811)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=4, iterations=300, minimum_probability=0.05, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.045*\"b\" + 0.042*\"onlin\" + 0.042*\"set\" + 0.033*\"price\" + 0.033*\"app\" + 0.028*\"end\" + 0.027*\"alreadi\" + 0.024*\"x\" + 0.021*\"pin\" + 0.021*\"button\"\n",
      "Topic: 1 \n",
      "Words: 0.179*\"plan\" + 0.079*\"unlimit\" + 0.068*\"data\" + 0.063*\"time\" + 0.062*\"one\" + 0.037*\"gb\" + 0.036*\"go\" + 0.034*\"card\" + 0.031*\"chang\" + 0.029*\"question\"\n",
      "Topic: 2 \n",
      "Words: 0.174*\"today\" + 0.133*\"order\" + 0.074*\"place\" + 0.034*\"avail\" + 0.032*\"need\" + 0.020*\"id\" + 0.019*\"issu\" + 0.019*\"black\" + 0.017*\"year\" + 0.017*\"name\"\n",
      "Topic: 3 \n",
      "Words: 0.173*\"line\" + 0.091*\"fee\" + 0.056*\"charg\" + 0.046*\"upgrad\" + 0.042*\"activ\" + 0.041*\"right\" + 0.038*\"access\" + 0.037*\"option\" + 0.033*\"per\" + 0.027*\"sorri\"\n",
      "Topic: 4 \n",
      "Words: 0.117*\"bill\" + 0.112*\"new\" + 0.112*\"devic\" + 0.079*\"payment\" + 0.041*\"monthli\" + 0.033*\"current\" + 0.031*\"may\" + 0.029*\"credit\" + 0.028*\"call\" + 0.028*\"cycl\"\n",
      "Topic: 5 \n",
      "Words: 0.083*\"n\" + 0.070*\"month\" + 0.056*\"sure\" + 0.055*\"pleas\" + 0.034*\"email\" + 0.032*\"last\" + 0.030*\"provid\" + 0.029*\"address\" + 0.027*\"good\" + 0.026*\"sent\"\n",
      "Topic: 6 \n",
      "Words: 0.229*\"account\" + 0.054*\"access\" + 0.052*\"pin\" + 0.050*\"iphon\" + 0.046*\"secur\" + 0.046*\"code\" + 0.037*\"digit\" + 0.031*\"number\" + 0.028*\"br\" + 0.025*\"verif\"\n",
      "Topic: 7 \n",
      "Words: 0.175*\"verizon\" + 0.101*\"chat\" + 0.074*\"sale\" + 0.070*\"wireless\" + 0.056*\"servic\" + 0.045*\"custom\" + 0.030*\"store\" + 0.029*\"text\" + 0.025*\"messag\" + 0.025*\"team\"\n",
      "Topic: 8 \n",
      "Words: 0.065*\"next\" + 0.064*\"discount\" + 0.063*\"bill\" + 0.057*\"due\" + 0.046*\"total\" + 0.040*\"pay\" + 0.036*\"’\" + 0.034*\"usag\" + 0.034*\"mobil\" + 0.030*\"protect\"\n",
      "Topic: 9 \n",
      "Words: 0.229*\"phone\" + 0.091*\"number\" + 0.051*\"day\" + 0.040*\"like\" + 0.034*\"trade\" + 0.032*\"well\" + 0.029*\"use\" + 0.021*\"look\" + 0.021*\"back\" + 0.020*\"old\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.056*\"digit\" + 0.048*\"secur\" + 0.041*\"code\" + 0.037*\"number\" + 0.032*\"verif\" + 0.030*\"pin\" + 0.028*\"mobil\" + 0.026*\"account\" + 0.020*\"discount\" + 0.019*\"free\"\n",
      "Topic: 1 Word: 0.053*\"servic\" + 0.049*\"sorri\" + 0.043*\"date\" + 0.038*\"n\" + 0.037*\"know\" + 0.034*\"trade\" + 0.031*\"activ\" + 0.029*\"may\" + 0.021*\"today\" + 0.017*\"success\"\n",
      "Topic: 2 Word: 0.085*\"plan\" + 0.079*\"unlimit\" + 0.038*\"go\" + 0.034*\"gb\" + 0.031*\"data\" + 0.027*\"verizon\" + 0.026*\"store\" + 0.023*\"account\" + 0.020*\"right\" + 0.020*\"intern\"\n",
      "Topic: 3 Word: 0.054*\"line\" + 0.042*\"phone\" + 0.035*\"credit\" + 0.031*\"fee\" + 0.031*\"per\" + 0.029*\"return\" + 0.023*\"access\" + 0.019*\"issu\" + 0.018*\"bill\" + 0.017*\"via\"\n",
      "Topic: 4 Word: 0.064*\"time\" + 0.054*\"payment\" + 0.040*\"ask\" + 0.030*\"may\" + 0.028*\"pass\" + 0.026*\"travel\" + 0.021*\"set\" + 0.018*\"week\" + 0.015*\"one\" + 0.014*\"devic\"\n",
      "Topic: 5 Word: 0.082*\"bill\" + 0.055*\"chat\" + 0.054*\"verizon\" + 0.035*\"wireless\" + 0.031*\"sale\" + 0.023*\"button\" + 0.022*\"address\" + 0.021*\"cycl\" + 0.018*\"patient\" + 0.016*\"new\"\n",
      "Topic: 6 Word: 0.072*\"’\" + 0.054*\"b\" + 0.041*\"number\" + 0.039*\"wireless\" + 0.036*\"sale\" + 0.033*\"verizon\" + 0.032*\"chat\" + 0.020*\"need\" + 0.020*\"sure\" + 0.019*\"detail\"\n",
      "Topic: 7 Word: 0.034*\"phone\" + 0.033*\"new\" + 0.032*\"verizon\" + 0.026*\"one\" + 0.025*\"support\" + 0.023*\"servic\" + 0.023*\"usag\" + 0.021*\"line\" + 0.020*\"text\" + 0.019*\"hour\"\n",
      "Topic: 8 Word: 0.040*\"charg\" + 0.036*\"day\" + 0.030*\"pay\" + 0.029*\"account\" + 0.029*\"auto\" + 0.028*\"due\" + 0.027*\"still\" + 0.024*\"balanc\" + 0.021*\"cell\" + 0.021*\"everyth\"\n",
      "Topic: 9 Word: 0.033*\"famili\" + 0.031*\"smart\" + 0.028*\"want\" + 0.023*\"away\" + 0.021*\"verizon\" + 0.020*\"access\" + 0.019*\"promot\" + 0.018*\"right\" + 0.018*\"help\" + 0.016*\"question\"\n",
      "Topic: 10 Word: 0.080*\"order\" + 0.062*\"place\" + 0.054*\"today\" + 0.032*\"custom\" + 0.027*\"care\" + 0.023*\"iphon\" + 0.021*\"case\" + 0.021*\"xs\" + 0.020*\"next\" + 0.020*\"assur\"\n",
      "Topic: 11 Word: 0.121*\"devic\" + 0.047*\"payment\" + 0.036*\"sinc\" + 0.032*\"bill\" + 0.028*\"current\" + 0.022*\"paid\" + 0.020*\"b\" + 0.019*\"good\" + 0.019*\"plan\" + 0.018*\"get\"\n",
      "Topic: 12 Word: 0.083*\"phone\" + 0.063*\"number\" + 0.039*\"check\" + 0.038*\"call\" + 0.029*\"cycl\" + 0.027*\"pleas\" + 0.023*\"differ\" + 0.021*\"bill\" + 0.018*\"next\" + 0.016*\"plan\"\n",
      "Topic: 13 Word: 0.084*\"new\" + 0.033*\"screen\" + 0.033*\"amount\" + 0.029*\"email\" + 0.028*\"account\" + 0.023*\"text\" + 0.022*\"messag\" + 0.019*\"sent\" + 0.019*\"devic\" + 0.017*\"line\"\n",
      "Topic: 14 Word: 0.065*\"data\" + 0.046*\"card\" + 0.037*\"sure\" + 0.037*\"much\" + 0.030*\"sim\" + 0.028*\"team\" + 0.023*\"welcom\" + 0.020*\"usag\" + 0.017*\"end\" + 0.015*\"u\"\n",
      "Topic: 15 Word: 0.084*\"account\" + 0.060*\"access\" + 0.052*\"glad\" + 0.029*\"app\" + 0.026*\"phone\" + 0.022*\"line\" + 0.022*\"verizon\" + 0.019*\"avail\" + 0.017*\"data\" + 0.017*\"allow\"\n",
      "Topic: 16 Word: 0.057*\"like\" + 0.038*\"next\" + 0.035*\"bill\" + 0.032*\"discount\" + 0.029*\"n\" + 0.025*\"look\" + 0.024*\"name\" + 0.022*\"use\" + 0.021*\"thing\" + 0.020*\"charg\"\n",
      "Topic: 17 Word: 0.047*\"one\" + 0.035*\"today\" + 0.033*\"provid\" + 0.033*\"plan\" + 0.032*\"resolut\" + 0.032*\"addit\" + 0.032*\"charg\" + 0.028*\"time\" + 0.024*\"question\" + 0.024*\"old\"\n",
      "Topic: 18 Word: 0.059*\"line\" + 0.049*\"well\" + 0.048*\"lose\" + 0.035*\"fee\" + 0.033*\"monthli\" + 0.029*\"upgrad\" + 0.021*\"last\" + 0.021*\"ok\" + 0.019*\"credit\" + 0.019*\"access\"\n",
      "Topic: 19 Word: 0.049*\"month\" + 0.034*\"last\" + 0.028*\"see\" + 0.028*\"alreadi\" + 0.027*\"protect\" + 0.024*\"total\" + 0.023*\"usual\" + 0.022*\"mobil\" + 0.022*\"appl\" + 0.019*\"previou\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=20, id2word=dictionary, passes=2, workers=4, iterations=300, minimum_probability=0.05, random_state=2018)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[1]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.41746844602061717\t Topic: 0.065*\"next\" + 0.064*\"discount\" + 0.063*\"bill\" + 0.057*\"due\" + 0.046*\"total\" + 0.040*\"pay\" + 0.036*\"’\" + 0.034*\"usag\" + 0.034*\"mobil\" + 0.030*\"protect\"\n",
      "Score: 0.18252313698927794\t Topic: 0.173*\"line\" + 0.091*\"fee\" + 0.056*\"charg\" + 0.046*\"upgrad\" + 0.042*\"activ\" + 0.041*\"right\" + 0.038*\"access\" + 0.037*\"option\" + 0.033*\"per\" + 0.027*\"sorri\"\n",
      "Score: 0.15714714844977146\t Topic: 0.229*\"phone\" + 0.091*\"number\" + 0.051*\"day\" + 0.040*\"like\" + 0.034*\"trade\" + 0.032*\"well\" + 0.029*\"use\" + 0.021*\"look\" + 0.021*\"back\" + 0.020*\"old\"\n",
      "Score: 0.15714296428120247\t Topic: 0.045*\"b\" + 0.042*\"onlin\" + 0.042*\"set\" + 0.033*\"price\" + 0.033*\"app\" + 0.028*\"end\" + 0.027*\"alreadi\" + 0.024*\"x\" + 0.021*\"pin\" + 0.021*\"button\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'Why should I pay extra $110 even though I haven\\'t used any data'\n",
    "bow_corpus1 = dictionary.doc2bow(CleanUp(unseen_document))\n",
    "#tfidf = models.TfidfModel(bow_corpus)\n",
    "#corpus_tfidf = tfidf[bow_corpus]\n",
    "for index, score in sorted(lda_model[bow_corpus1], key=lambda tup: -1*tup[1]):\n",
    "    #print(score, index)\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
